{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phrase Learning\n",
    "\n",
    "Phrase learning is a very common problem that arises when we deal with text data. In this notebook, we will use PySpark to build a phrase learner using Stackoverflow's dataset containing Javascript posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "We use [Spark's NLP library](https://nlp.johnsnowlabs.com/) for detecting sentences and normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, pandas_udf, PandasUDFType, col, split, log\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, NGram\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data\n",
    "\n",
    "Before executing the below command, download the files, untar and place them in the working directory - [questions](https://bostondata.blob.core.windows.net/stackoverflow/orig-q.tsv.gz), [duplicates](https://bostondata.blob.core.windows.net/stackoverflow/dup-q.tsv.gz) and [answers](https://bostondata.blob.core.windows.net/stackoverflow/ans.tsv.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = spark.read.options(delimiter='\\t')\\\n",
    "                 .csv('orig-q.tsv')\\\n",
    "                 .toDF('id', 'ans_id', 'text', 'date')\n",
    "\n",
    "duplicates = spark.read.options(delimiter='\\t')\\\n",
    "                  .csv('dup-q.tsv')\\\n",
    "                  .toDF('id', 'ans_id', 'text', 'date')\n",
    "\n",
    "answers = spark.read.options(delimiter='\\t')\\\n",
    "               .csv('ans.tsv')\\\n",
    "               .toDF('id', 'text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work on all the text data available in these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = questions.select('text').union(answers.select('text')).union(duplicates.select('text'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paragraph extraction from text\n",
    "\n",
    "The text of any post starts with the title and continues with the body. Usually, the body of the post consists of code blocks, quotes and paragraphs. Since the post has a structure that's enforced by html tags, our job of detecting paragraphs becomes a little easier.\n",
    "\n",
    "We use regex tokenizer to tokenize the post into paragraphs. Here is a sample post:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessing the web page's HTTP Headers in JavaScript. <p>How do I access a page's HTTP response headers via JavaScript?</p> <p>Related to <a href=\"http://stackoverflow.com/questions/220149/how-do-i-access-the-http-request-header-fields-via-javascript\"><strong>this question</strong></a>, which was modified to ask about accessing two specific HTTP headers.</p> <blockquote> <p><strong>Related:</strong><br> <a href=\"http://stackoverflow.com/questions/220149/how-do-i-access-the-http-request-header-fields-via-javascript\">How do I access the HTTP request header fields via JavaScript?</a></p> </blockquote>\n"
     ]
    }
   ],
   "source": [
    "print(df.take(1)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark RegexTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_tokenizer = RegexTokenizer(pattern='(<p>(.*?)<\\/p>)+', gaps=False, inputCol='text', outputCol='paragraphs')\n",
    "\n",
    "# first tokenize post to sentences, then return a new row for each sentence in the post\n",
    "df = regex_tokenizer.transform(df)\\\n",
    "                    .select(explode('paragraphs').alias('paragraphs'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean body of post\n",
    "\n",
    "Strip text of code blocks, html tags and urls/links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf('string', PandasUDFType.SCALAR)\n",
    "def clean_text(s):\n",
    "    return s.str.strip()\\\n",
    "            .str.replace('<pre><code>.*?</code></pre>|<[^>]+>|<a[^>]+>(.*)</a>|', '')\n",
    "\n",
    "df = df.withColumn('clean_paragraphs', clean_text(col('paragraphs')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|          paragraphs|    clean_paragraphs|\n",
      "+--------------------+--------------------+\n",
      "|<p>how do i acces...|how do i access a...|\n",
      "|<p>related to <a ...|related to this q...|\n",
      "|<p><strong>relate...|related: how do i...|\n",
      "|<p>i need to some...|i need to somehow...|\n",
      "|   <p>any ideas?</p>|          any ideas?|\n",
      "|<p>i'm not agains...|i'm not against u...|\n",
      "|<p>i am using <co...|i am using setint...|\n",
      "|<p>i want the use...|i want the user t...|\n",
      "|<p>how can an ema...|how can an email ...|\n",
      "|<p>suppose i atta...|suppose i attach ...|\n",
      "|<p>is there a way...|is there a way to...|\n",
      "|<p>for example, s...|for example, supp...|\n",
      "|<p>if i click the...|if i click the sp...|\n",
      "|<p>ps: if the onc...|ps: if the onclic...|\n",
      "|<p>pps: the backg...|pps: the backgrou...|\n",
      "|<p>i'm writing a ...|i'm writing a web...|\n",
      "|<p>is it possible...|is it possible to...|\n",
      "|<p>so the smes at...|so the smes at my...|\n",
      "|<p>what the users...|what the users ha...|\n",
      "|<p>i know there a...|i know there are ...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence boundary detection\n",
    "\n",
    "We use the [Spark-NLP library](https://nlp.johnsnowlabs.com/) to detect sentence boundaries in paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_assembler = DocumentAssembler()\\\n",
    "  .setInputCol(\"clean_paragraphs\")\\\n",
    "  .setOutputCol(\"document\")\n",
    "\n",
    "sentence_detector = SentenceDetector()\\\n",
    "  .setInputCols([\"document\"])\\\n",
    "  .setOutputCol(\"sent\")\n",
    "\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"sent\"]) \\\n",
    "    .setIncludeKeys(False) \\\n",
    "    .setCleanAnnotations(True)\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages = [\n",
    "    document_assembler,\n",
    "    sentence_detector,\n",
    "    finisher\n",
    "  ])\n",
    "\n",
    "df = pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|          paragraphs|    clean_paragraphs|       finished_sent|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|<p>how do i acces...|how do i access a...|how do i access a...|\n",
      "|<p>related to <a ...|related to this q...|related to this q...|\n",
      "|<p><strong>relate...|related: how do i...|related: how do i...|\n",
      "|<p>i need to some...|i need to somehow...|i need to somehow...|\n",
      "|   <p>any ideas?</p>|          any ideas?|          any ideas?|\n",
      "|<p>i'm not agains...|i'm not against u...|i'm not against u...|\n",
      "|<p>i am using <co...|i am using setint...|i am using setint...|\n",
      "|<p>i want the use...|i want the user t...|i want the user t...|\n",
      "|<p>how can an ema...|how can an email ...|how can an email ...|\n",
      "|<p>suppose i atta...|suppose i attach ...|suppose i attach ...|\n",
      "|<p>is there a way...|is there a way to...|is there a way to...|\n",
      "|<p>for example, s...|for example, supp...|for example, supp...|\n",
      "|<p>if i click the...|if i click the sp...|if i click the sp...|\n",
      "|<p>ps: if the onc...|ps: if the onclic...|ps: if the onclic...|\n",
      "|<p>pps: the backg...|pps: the backgrou...|pps: the backgrou...|\n",
      "|<p>i'm writing a ...|i'm writing a web...|i'm writing a web...|\n",
      "|<p>is it possible...|is it possible to...|is it possible to...|\n",
      "|<p>so the smes at...|so the smes at my...|so the smes at my...|\n",
      "|<p>what the users...|what the users ha...|what the users ha...|\n",
      "|<p>i know there a...|i know there are ...|i know there are ...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and Splitting Sentences\n",
    "\n",
    "This is basically an implementation of what's already done [here](https://github.com/Azure/MachineLearningSamples-QnAMatching/blob/master/modules/phrase_learning.py)\n",
    "\n",
    "#### Regex to split on punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_tokenizer = RegexTokenizer(pattern=\"[\\\"\\!\\?\\)\\]\\}\\,\\:\\;\\*\\-]*\\s+\\([0-9]+\\)\\s+[\\(\\[\\{\\\"\\*\\-]*\"                         \n",
    "                                               \"|[\\\"\\!\\?\\)\\]\\}\\,\\:\\;\\*\\-]+\\s+[\\(\\[\\{\\\"\\*\\-]*\" \n",
    "                                               \"|\\.\\.+\"       # ..\n",
    "                                               \"|\\s*\\-\\-+\\s*\" # --\n",
    "                                               \"|\\s+\\-\\s+\"    # -  \n",
    "                                               \"|\\:\\:+\"       # ::\n",
    "                                               \"|\\s+[\\/\\(\\[\\{\\\"\\-\\*]+\\s*\"  \n",
    "                                               \"|[\\,!\\?\\\"\\)\\(\\]\\[\\}\\{\\:\\;\\*](?=[a-zA-Z])\"\n",
    "                                               \"|[\\\"\\!\\?\\)\\]\\}\\,\\:\\;]+[\\.]*$\",\n",
    "                                       inputCol='finished_sent',\n",
    "                                       outputCol='sent_sans_punct')\n",
    "\n",
    "df = punctuation_tokenizer.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|          paragraphs|    clean_paragraphs|       finished_sent|     sent_sans_punct|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|<p>how do i acces...|how do i access a...|how do i access a...|[how do i access ...|\n",
      "|<p>related to <a ...|related to this q...|related to this q...|[related to this ...|\n",
      "|<p><strong>relate...|related: how do i...|related: how do i...|[related, how do ...|\n",
      "|<p>i need to some...|i need to somehow...|i need to somehow...|[i need to someho...|\n",
      "|   <p>any ideas?</p>|          any ideas?|          any ideas?|         [any ideas]|\n",
      "|<p>i'm not agains...|i'm not against u...|i'm not against u...|[i'm not against ...|\n",
      "|<p>i am using <co...|i am using setint...|i am using setint...|[i am using setin...|\n",
      "|<p>i want the use...|i want the user t...|i want the user t...|[i want the user ...|\n",
      "|<p>how can an ema...|how can an email ...|how can an email ...|[how can an email...|\n",
      "|<p>suppose i atta...|suppose i attach ...|suppose i attach ...|[suppose i attach...|\n",
      "|<p>is there a way...|is there a way to...|is there a way to...|[is there a way t...|\n",
      "|<p>for example, s...|for example, supp...|for example, supp...|[for example, sup...|\n",
      "|<p>if i click the...|if i click the sp...|if i click the sp...|[if i click the s...|\n",
      "|<p>ps: if the onc...|ps: if the onclic...|ps: if the onclic...|[ps, if the oncli...|\n",
      "|<p>pps: the backg...|pps: the backgrou...|pps: the backgrou...|[pps, the backgro...|\n",
      "|<p>i'm writing a ...|i'm writing a web...|i'm writing a web...|[i'm writing a we...|\n",
      "|<p>is it possible...|is it possible to...|is it possible to...|[is it possible t...|\n",
      "|<p>so the smes at...|so the smes at my...|so the smes at my...|[so the smes at m...|\n",
      "|<p>what the users...|what the users ha...|what the users ha...|[what the users h...|\n",
      "|<p>i know there a...|i know there are ...|i know there are ...|[i know there are...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(explode(col('sent_sans_punct')).alias('sentences'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           sentences|\n",
      "+--------------------+\n",
      "|how do i access a...|\n",
      "|related to this q...|\n",
      "|which was modifie...|\n",
      "|             related|\n",
      "|how do i access t...|\n",
      "|i need to somehow...|\n",
      "|       not even ssi.|\n",
      "|           any ideas|\n",
      "|i'm not against u...|\n",
      "|if someone can su...|\n",
      "|i am using setint...|\n",
      "|               fname|\n",
      "|10000);@to call a...|\n",
      "|i want the user t...|\n",
      "|how can an email ...|\n",
      "|suppose i attach ...|\n",
      "|is there a way to...|\n",
      "|the element which...|\n",
      "|inside the functi...|\n",
      "|         for example|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove underbars, equal signs and parenthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace underbars with spaces\n",
    "@pandas_udf('string', PandasUDFType.SCALAR)\n",
    "def underbar_to_spaces(s):\n",
    "    return s.str.strip()\\\n",
    "            .str.replace('_|_+', ' ')\n",
    "\n",
    "df = df.withColumn('sentences_without_underbars', underbar_to_spaces('sentences'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------------+\n",
      "|           sentences|sentences_without_underbars|\n",
      "+--------------------+---------------------------+\n",
      "|how do i access a...|       how do i access a...|\n",
      "|related to this q...|       related to this q...|\n",
      "|which was modifie...|       which was modifie...|\n",
      "|             related|                    related|\n",
      "|how do i access t...|       how do i access t...|\n",
      "|i need to somehow...|       i need to somehow...|\n",
      "|       not even ssi.|              not even ssi.|\n",
      "|           any ideas|                  any ideas|\n",
      "|i'm not against u...|       i'm not against u...|\n",
      "|if someone can su...|       if someone can su...|\n",
      "|i am using setint...|       i am using setint...|\n",
      "|               fname|                      fname|\n",
      "|10000);@to call a...|       10000);@to call a...|\n",
      "|i want the user t...|       i want the user t...|\n",
      "|how can an email ...|       how can an email ...|\n",
      "|suppose i attach ...|       suppose i attach ...|\n",
      "|is there a way to...|       is there a way to...|\n",
      "|the element which...|       the element which...|\n",
      "|inside the functi...|       inside the functi...|\n",
      "|         for example|                for example|\n",
      "+--------------------+---------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"sentences_without_underbars\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "  .setInputCols([\"document\"]) \\\n",
    "  .setOutputCol(\"token\")\n",
    "\n",
    "normalizer = Normalizer() \\\n",
    "  .setInputCols([\"token\"]) \\\n",
    "  .setOutputCol(\"normalized\") \\\n",
    "  .setPatterns([\"_+|\\(\\$?|=|\\)\\$?\"])\n",
    "\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"normalized\"]) \\\n",
    "    .setIncludeKeys(False) \\\n",
    "    .setCleanAnnotations(True)\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages = [\n",
    "    document_assembler,\n",
    "    tokenizer,\n",
    "    normalizer,\n",
    "    finisher\n",
    "  ])\n",
    "\n",
    "df = pipeline.fit(df).transform(df)\n",
    "\n",
    "# split on @ to get multiple tokens\n",
    "df = df.withColumn('tokens', split(df['finished_normalized'], '@'))\n",
    "\n",
    "# remove stopwords\n",
    "stopwords_remover = StopWordsRemover(inputCol='tokens', outputCol='clean_tokens')\n",
    "\n",
    "df = stopwords_remover.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------------+--------------------+--------------------+--------------------+\n",
      "|           sentences|sentences_without_underbars| finished_normalized|              tokens|        clean_tokens|\n",
      "+--------------------+---------------------------+--------------------+--------------------+--------------------+\n",
      "|how do i access a...|       how do i access a...|how@do@i@access@a...|[how, do, i, acce...|[access, page, 's...|\n",
      "|related to this q...|       related to this q...|related@to@this@q...|[related, to, thi...| [related, question]|\n",
      "|which was modifie...|       which was modifie...|which@was@modifie...|[which, was, modi...|[modified, ask, a...|\n",
      "|             related|                    related|             related|           [related]|           [related]|\n",
      "|how do i access t...|       how do i access t...|how@do@i@access@t...|[how, do, i, acce...|[access, http, re...|\n",
      "|i need to somehow...|       i need to somehow...|i@need@to@somehow...|[i, need, to, som...|[need, somehow, p...|\n",
      "|       not even ssi.|              not even ssi.|      not@even@ssi@.| [not, even, ssi, .]|      [even, ssi, .]|\n",
      "|           any ideas|                  any ideas|           any@ideas|        [any, ideas]|             [ideas]|\n",
      "|i'm not against u...|       i'm not against u...|i@'m@not@against@...|[i, 'm, not, agai...|['m, using, free,...|\n",
      "|if someone can su...|       if someone can su...|if@someone@can@su...|[if, someone, can...|[someone, suggest...|\n",
      "|i am using setint...|       i am using setint...|i@am@using@setint...|[i, am, using, se...|[using, setinterval]|\n",
      "|               fname|                      fname|               fname|             [fname]|             [fname]|\n",
      "|10000);@to call a...|       10000);@to call a...|10000;@to@call@a@...|[10000;, to, call...|[10000;, call, fu...|\n",
      "|i want the user t...|       i want the user t...|i@want@the@user@t...|[i, want, the, us...|[want, user, able...|\n",
      "|how can an email ...|       how can an email ...|how@can@an@email@...|[how, can, an, em...|[email, address, ...|\n",
      "|suppose i attach ...|       suppose i attach ...|suppose@i@attach@...|[suppose, i, atta...|[suppose, attach,...|\n",
      "|is there a way to...|       is there a way to...|is@there@a@way@to...|[is, there, a, wa...|[way, get, id, el...|\n",
      "|the element which...|       the element which...|the@element@which...|[the, element, wh...|  [element, clicked]|\n",
      "|inside the functi...|       inside the functi...|inside@the@functi...|[inside, the, fun...| [inside, function?]|\n",
      "|         for example|                for example|         for@example|      [for, example]|           [example]|\n",
      "+--------------------+---------------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized Pointwise Mutual Information\n",
    "\n",
    "For finding commonly used bigram phrases, we will use Normalized Pointwise Mutual Information to rank the phrases. For two words $x$ and $y$, the normalized pointwise mutual information (nmpi), $nmpi(x,y)$ is defined as:\n",
    "\n",
    "$$nmpi(x,y) = -\\frac{1}{\\ln{p(x, y)}} \\cdot \\ln{\\frac{p(x, y)}{p(x)p(y)}}$$\n",
    "\n",
    "$p(x)$ and $p(y)$ represent probabilities of $x$ and $y$. Ideally, we must use smoothed probabilities but for simplicity, we will use counts here.\n",
    "\n",
    "Read more about NMPI [here](https://svn.spraakdata.gu.se/repos/gerlof/pub/www/Docs/npmi-pfd.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unigram counts\n",
    "unigrams = df.select(explode('clean_tokens').alias('unigram')).groupBy('unigram').count()\n",
    "\n",
    "# bigram counts\n",
    "ngrams = NGram(n=2, inputCol='clean_tokens', outputCol='ngram')\n",
    "bigrams = ngrams.transform(df).select(explode('ngram').alias('bigram')).groupBy('bigram').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get individual words for the ngram\n",
    "split_col = split(col('bigram'), ' ')\n",
    "bigrams = bigrams.select('bigram', col('count').alias('bigram_count'),\n",
    "                         split_col.getItem(0).alias('word1'),\n",
    "                         split_col.getItem(1).alias('word2'))\n",
    "\n",
    "# get count of word1\n",
    "bigrams = bigrams.join(unigrams, bigrams['word1'] == unigrams['unigram'])\\\n",
    "                 .select('bigram', 'bigram_count', col('count').alias('word1_count'), 'word2')\n",
    "\n",
    "bigrams = bigrams.join(unigrams, bigrams['word2'] == unigrams['unigram'])\\\n",
    "                 .select('bigram', 'bigram_count', 'word1_count',\n",
    "                         col('count').alias('word2_count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[bigram: string, bigram_count: bigint, word1_count: bigint, word2_count: bigint]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = bigrams.groupBy().sum('bigram_count').collect()[0][0]\n",
    "\n",
    "bigrams = bigrams.withColumn('npmi',\n",
    "    log(2.0, col('bigram_count') * N / col('word1_count') / col('word2_count')) / -log(2.0, col('bigram_count')/N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = bigrams.filter(col('bigram_count') > 20).orderBy(-col('npmi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-----------+-----------+------------------+\n",
      "|              bigram|bigram_count|word1_count|word2_count|              npmi|\n",
      "+--------------------+------------+-----------+-----------+------------------+\n",
      "|   douglas crockford|          22|         23|         46|0.9235799177561944|\n",
      "|                & lt|         137|        268|        157|0.9039493435633289|\n",
      "|          john resig|          23|         65|         23|0.8980390219748962|\n",
      "|  possible duplicate|        2350|       3778|       2615|0.8954366413246286|\n",
      "|    doctype html&gt;|          34|         44|         76| 0.891592775739534|\n",
      "|  uncaught typeerror|         155|        262|        237| 0.885336038746305|\n",
      "|   internet explorer|          89|        222|        104|0.8789273729401587|\n",
      "|    unexpected token|          75|        129|        132|0.8770265244977943|\n",
      "|  requested resource|          60|         91|        127|0.8736378242708454|\n",
      "|      stack overflow|          59|        107|        106|0.8722631430682533|\n",
      "|  same-origin policy|          28|         33|         89|0.8678262387505977|\n",
      "|&ldquo; equal&rdquo;|          32|        118|         32|0.8676372561503368|\n",
      "|     document. ready|          97|        138|        224|0.8640582419767608|\n",
      "|         dollar sign|          32|         41|        104|0.8553090394184731|\n",
      "|   present requested|          60|        166|         91|0.8446245240891836|\n",
      "|    exclamation mark|          27|         41|         88|0.8405354464867695|\n",
      "|         came across|         121|        218|        267|0.8381767489876187|\n",
      "|  latitude longitude|          28|         65|         62|0.8361644469867509|\n",
      "|parentheses surro...|          23|        115|         32|0.8096323502738267|\n",
      "| greatly appreciated|         109|        140|        472|0.8012426848360472|\n",
      "+--------------------+------------+-----------+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
